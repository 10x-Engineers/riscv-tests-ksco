
# See LICENSE for license details.

# This file is automatically generated. Do not edit.

#*****************************************************************************
# vse64.v_LMUL8.S
#-----------------------------------------------------------------------------
#
# Test vse64.v insnructions.
# With LMUL=8
#

#include "riscv_test.h"
#include "test_macros.h"

RVTEST_RV64UV

RVTEST_CODE_BEGIN


  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  la a2, tdat
  mv s1, a2
  addi a2, a2, 8
  vle64.v v8, (a2)
  la a1, res
  vse64.v v8, (a1)
  vle64.v v8, (s1)

  
  li t0, 16
  vsetvli t1, t0, e64,m8,ta,ma
  vse64.v v8, (a1)

  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  vle64.v v8, (a1)

  addi x0, x8, 16
  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  la a2, tdat
  mv s1, a2
  addi a2, a2, 8
  vle64.v v8, (a2)
  la a1, res
  vse64.v v8, (a1)
  vle64.v v8, (s1)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 16
  vsetvli t1, t0, e64,m8,ta,ma
  vse64.v v8, (a1), v0.t

  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  vle64.v v8, (a1)

  addi x0, x8, 16
  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  la a2, tdat
  mv s1, a2
  addi a2, a2, 8
  vle64.v v8, (a2)
  la a1, res
  vse64.v v8, (a1)
  vle64.v v8, (s1)

  
  li t0, 16
  vsetvli t1, t0, e64,m8,tu,ma
  vse64.v v8, (a1)

  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  vle64.v v8, (a1)

  addi x0, x8, 16
  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  la a2, tdat
  mv s1, a2
  addi a2, a2, 8
  vle64.v v8, (a2)
  la a1, res
  vse64.v v8, (a1)
  vle64.v v8, (s1)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 16
  vsetvli t1, t0, e64,m8,ta,mu
  vse64.v v8, (a1), v0.t

  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  vle64.v v8, (a1)

  addi x0, x8, 16
  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  la a2, tdat
  mv s1, a2
  addi a2, a2, 8
  vle64.v v8, (a2)
  la a1, res
  vse64.v v8, (a1)
  vle64.v v8, (s1)

  
  li t0, 31
  vsetvli t1, t0, e64,m8,ta,ma
  vse64.v v8, (a1)

  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  vle64.v v8, (a1)

  addi x0, x8, 16
  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  la a2, tdat
  mv s1, a2
  addi a2, a2, 8
  vle64.v v8, (a2)
  la a1, res
  vse64.v v8, (a1)
  vle64.v v8, (s1)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 31
  vsetvli t1, t0, e64,m8,ta,ma
  vse64.v v8, (a1), v0.t

  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  vle64.v v8, (a1)

  addi x0, x8, 16
  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  la a2, tdat
  mv s1, a2
  addi a2, a2, 8
  vle64.v v8, (a2)
  la a1, res
  vse64.v v8, (a1)
  vle64.v v8, (s1)

  
  li t0, 31
  vsetvli t1, t0, e64,m8,tu,ma
  vse64.v v8, (a1)

  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  vle64.v v8, (a1)

  addi x0, x8, 16
  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  la a2, tdat
  mv s1, a2
  addi a2, a2, 8
  vle64.v v8, (a2)
  la a1, res
  vse64.v v8, (a1)
  vle64.v v8, (s1)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 31
  vsetvli t1, t0, e64,m8,ta,mu
  vse64.v v8, (a1), v0.t

  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  vle64.v v8, (a1)

  addi x0, x8, 16
  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  la a2, tdat
  mv s1, a2
  addi a2, a2, 8
  vle64.v v8, (a2)
  la a1, res
  vse64.v v8, (a1)
  vle64.v v8, (s1)

  
  li t0, 32
  vsetvli t1, t0, e64,m8,ta,ma
  vse64.v v8, (a1)

  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  vle64.v v8, (a1)

  addi x0, x8, 16
  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  la a2, tdat
  mv s1, a2
  addi a2, a2, 8
  vle64.v v8, (a2)
  la a1, res
  vse64.v v8, (a1)
  vle64.v v8, (s1)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 32
  vsetvli t1, t0, e64,m8,ta,ma
  vse64.v v8, (a1), v0.t

  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  vle64.v v8, (a1)

  addi x0, x8, 16
  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  la a2, tdat
  mv s1, a2
  addi a2, a2, 8
  vle64.v v8, (a2)
  la a1, res
  vse64.v v8, (a1)
  vle64.v v8, (s1)

  
  li t0, 32
  vsetvli t1, t0, e64,m8,tu,ma
  vse64.v v8, (a1)

  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  vle64.v v8, (a1)

  addi x0, x8, 16
  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  la a2, tdat
  mv s1, a2
  addi a2, a2, 8
  vle64.v v8, (a2)
  la a1, res
  vse64.v v8, (a1)
  vle64.v v8, (s1)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 32
  vsetvli t1, t0, e64,m8,ta,mu
  vse64.v v8, (a1), v0.t

  li t0, -1
  vsetvli t1, t0, e64,m8,ta,ma
  vle64.v v8, (a1)

  addi x0, x8, 16

  TEST_CASE(2, x0, 0x0)
  TEST_PASSFAIL

RVTEST_CODE_END

  .data
RVTEST_DATA_BEGIN

res:
  .zero 264

tdat:
  .quad 0xbf8003044003b0f0
  .quad 0x40400000c0800000
  .quad 0xdeadbeefcafebabe
  .quad 0xabad1dea1337d00d
  .quad 0xbf8003044003b0f0
  .quad 0x40400000c0800000
  .quad 0xdeadbeefcafebabe
  .quad 0xabad1dea1337d00d
  .quad 0xbf8003044003b0f0
  .quad 0x40400000c0800000
  .quad 0xdeadbeefcafebabe
  .quad 0xabad1dea1337d00d
  .quad 0xbf8003044003b0f0
  .quad 0x40400000c0800000
  .quad 0xdeadbeefcafebabe
  .quad 0xabad1dea1337d00d
  .quad 0xbf8003044003b0f0
  .quad 0x40400000c0800000
  .quad 0xdeadbeefcafebabe
  .quad 0xabad1dea1337d00d
  .quad 0xbf8003044003b0f0
  .quad 0x40400000c0800000
  .quad 0xdeadbeefcafebabe
  .quad 0xabad1dea1337d00d
  .quad 0xbf8003044003b0f0
  .quad 0x40400000c0800000
  .quad 0xdeadbeefcafebabe
  .quad 0xabad1dea1337d00d
  .quad 0xbf8003044003b0f0
  .quad 0x40400000c0800000
  .quad 0xdeadbeefcafebabe
  .quad 0xabad1dea1337d00d
  .quad 0xbf8003044003b0f0

mask:
  .quad 0x5555555555555555
  .quad 0x5555555555555555
  .quad 0x5555555555555555
  .quad 0x5555555555555555

RVTEST_DATA_END
