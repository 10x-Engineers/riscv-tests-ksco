
# See LICENSE for license details.

# This file is automatically generated. Do not edit.

#*****************************************************************************
# vssrl.vv_LMUL1SEW64.S
#-----------------------------------------------------------------------------
#
# Test vssrl.vv insnructions.
# With LMUL=1, SEW=64
#

#include "riscv_test.h"
#include "test_macros.h"

RVTEST_RV64UV

RVTEST_CODE_BEGIN


  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, 2
  vsetvli t1, t0, e64,m1,ta,ma
  vssrl.vv v1, v2, v3

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)

  addi x0, x1, 2

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, 2
  vsetvli t1, t0, e64,m1,tu,ma
  vssrl.vv v1, v2, v3

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)

  addi x0, x1, 2

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 2
  vsetvli t1, t0, e64,m1,ta,ma
  vssrl.vv v1, v2, v3, v0.t

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)

  addi x0, x1, 2

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 2
  vsetvli t1, t0, e64,m1,ta,ma
  vssrl.vv v1, v2, v3, v0.t

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)

  addi x0, x1, 2

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, 3
  vsetvli t1, t0, e64,m1,ta,ma
  vssrl.vv v1, v2, v3

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)

  addi x0, x1, 2

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, 3
  vsetvli t1, t0, e64,m1,tu,ma
  vssrl.vv v1, v2, v3

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)

  addi x0, x1, 2

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 3
  vsetvli t1, t0, e64,m1,ta,ma
  vssrl.vv v1, v2, v3, v0.t

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)

  addi x0, x1, 2

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 3
  vsetvli t1, t0, e64,m1,ta,ma
  vssrl.vv v1, v2, v3, v0.t

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)

  addi x0, x1, 2

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, 4
  vsetvli t1, t0, e64,m1,ta,ma
  vssrl.vv v1, v2, v3

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)

  addi x0, x1, 2

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, 4
  vsetvli t1, t0, e64,m1,tu,ma
  vssrl.vv v1, v2, v3

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)

  addi x0, x1, 2

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 4
  vsetvli t1, t0, e64,m1,ta,ma
  vssrl.vv v1, v2, v3, v0.t

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)

  addi x0, x1, 2

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a2, tdat
  vle64.v v2, (a2)

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v1, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e64,m1,ta,ma
  vle64.v v3, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 4
  vsetvli t1, t0, e64,m1,ta,ma
  vssrl.vv v1, v2, v3, v0.t

  li t0, -1
  vsetvli t1, t0, e64,m1,ta,ma
  la a1, res
  vse64.v v1, (a1)

  addi x0, x1, 2


  TEST_CASE(2, x0, 0x0)
  TEST_PASSFAIL

RVTEST_CODE_END

  .data
RVTEST_DATA_BEGIN

res:
  .zero 144

tdat:
  .quad 0x0
  .quad 0x0
  .quad 0x1
  .quad 0x1
  .quad 0x3
  .quad 0x7
  .quad 0xfffffffffffffff8
  .quad 0x0
  .quad 0xffffffffffffffff

mask:
  .quad 0x5555555555555555
  .quad 0x5555555555555555
  .quad 0x5555555555555555
  .quad 0x5555555555555555

RVTEST_DATA_END
